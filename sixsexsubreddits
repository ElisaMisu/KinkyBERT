{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\n\n\n# Base directory\nbase_dir = '/kaggle/input/sixsexsubreddits'\n\n# Check the files in the dataset directory\nfiles = os.listdir(base_dir)\nprint(\"Files in dataset directory:\", files)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-03T20:31:48.671263Z","iopub.execute_input":"2024-07-03T20:31:48.671650Z","iopub.status.idle":"2024-07-03T20:31:48.678874Z","shell.execute_reply.started":"2024-07-03T20:31:48.671617Z","shell.execute_reply":"2024-07-03T20:31:48.677989Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Files in dataset directory: ['gonewildstories_submissions.json', 'Taboo_Confessions_submissions.json', 'confessions_submissions.json', 'My_Perverted_Stories_submissions.json', 'BDSMerotica_submissions.json', 'Erotica_submissions.json']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to read JSON Lines file\ndef read_json_lines(file_path):\n    data = []\n    with open(file_path, 'r') as file:\n        for line in file:\n            try:\n                data.append(json.loads(line))\n            except json.JSONDecodeError:\n                continue\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-07-03T20:34:32.154733Z","iopub.execute_input":"2024-07-03T20:34:32.155073Z","iopub.status.idle":"2024-07-03T20:34:32.160612Z","shell.execute_reply.started":"2024-07-03T20:34:32.155043Z","shell.execute_reply":"2024-07-03T20:34:32.159645Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Read all JSON files and find the 'selftext' column\nall_data = []\n\nfor file_name in files:\n    file_path = os.path.join(base_dir, file_name)\n    data = read_json_lines(file_path)\n    \n    df = pd.DataFrame(data)\n    \n    if 'selftext' in df.columns:\n        print(f\"'selftext' column found in {file_name}\")\n        all_data.append(df[['selftext']])\n    else:\n        print(f\"'selftext' column not found in {file_name}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T20:34:52.701425Z","iopub.execute_input":"2024-07-03T20:34:52.702014Z","iopub.status.idle":"2024-07-03T20:36:15.018272Z","shell.execute_reply.started":"2024-07-03T20:34:52.701980Z","shell.execute_reply":"2024-07-03T20:36:15.017233Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"'selftext' column found in gonewildstories_submissions.json\n'selftext' column found in Taboo_Confessions_submissions.json\n'selftext' column found in confessions_submissions.json\n'selftext' column found in My_Perverted_Stories_submissions.json\n'selftext' column found in BDSMerotica_submissions.json\n'selftext' column found in Erotica_submissions.json\n","output_type":"stream"}]},{"cell_type":"code","source":"# Concatenate all data into a single DataFrame\nif all_data:\n    combined_df = pd.concat(all_data, ignore_index=True)\n    print(f\"Combined DataFrame with 'selftext' column:\")\n    print(combined_df.head())\nelse:\n    combined_df = pd.DataFrame()\n    print(\"No 'selftext' column found in any file.\")\n\n# Display the final combined dataframe\ncombined_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T20:36:41.328638Z","iopub.execute_input":"2024-07-03T20:36:41.329131Z","iopub.status.idle":"2024-07-03T20:36:41.368167Z","shell.execute_reply.started":"2024-07-03T20:36:41.329093Z","shell.execute_reply":"2024-07-03T20:36:41.367261Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Combined DataFrame with 'selftext' column:\n                                            selftext\n0  Hi everyone, thanks for joining!  \\n\\nI'm look...\n1  My girlfriend and I were on vacation and the h...\n2  I'm a mostly straight guy, maybe 80/20. One of...\n3                                          [deleted]\n4  For the purposes of this story I have changed ...\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                            selftext\n0  Hi everyone, thanks for joining!  \\n\\nI'm look...\n1  My girlfriend and I were on vacation and the h...\n2  I'm a mostly straight guy, maybe 80/20. One of...\n3                                          [deleted]\n4  For the purposes of this story I have changed ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>selftext</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hi everyone, thanks for joining!  \\n\\nI'm look...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>My girlfriend and I were on vacation and the h...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I'm a mostly straight guy, maybe 80/20. One of...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[deleted]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>For the purposes of this story I have changed ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Correct installation of necessary packages\n!pip install zstandard bertopic sentence-transformers nltk ndjson\n\n# Import necessary libraries\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sentence_transformers import SentenceTransformer\nfrom bertopic import BERTopic\n\nTOKENIZERS_PARALLELISM=(False)\n\n# Define Reddit-specific terms\nreddit_terms = set([\n    'upvote', 'downvote', 'OP', 'TL;DR', 'tldr', 'edit', 'mods', 'moderator',\n    'AMA', 'ask me anything', 'crosspost', 'x-post', 'nsfw', 'flair', 'flairs',\n    'karma', 'subreddit', 'thread', 'username', 'usernames', 'selfpost', 'self-post',\n    'comment', 'comments', 'post', 'posts', 'reply', 'replies', 'vote', 'votes', 'deleted', 'delete',\n    'acct', 'expired', 'account', 'verified', 'reported', 'report', 'remove', 'removed',\n    'modmail', 'combined', 'submissions'\n])\n\ndef minimal_preprocess(text):\n    if isinstance(text, str):\n        # Remove URLs\n        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n        # Remove special characters and digits\n        text = re.sub(r'\\@\\w+|\\#', '', text)\n        text = re.sub(r'[^A-Za-z\\s]', '', text)\n        # Convert to lowercase\n        text = text.lower()\n        return text\n    else:\n        return \"\"\n\ndef full_preprocess(text):\n    if isinstance(text, str):\n        # Remove URLs\n        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n        # Remove special characters and digits\n        text = re.sub(r'\\@\\w+|\\#', '', text)\n        text = re.sub(r'[^A-Za-z\\s]', '', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords and common Reddit terms\n        stop_words = set(stopwords.words('english'))\n        text = ' '.join(word for word in text.split() if word not in stop_words and word not in reddit_terms)\n        return text\n    else:\n        return \"\"\n\n# Apply minimal preprocessing to create embeddings\ncombined_df['minimal_processed_text'] = combined_df['selftext'].apply(minimal_preprocess)\n\n# Filter out empty minimally processed texts\ncombined_df = combined_df[combined_df['minimal_processed_text'].apply(lambda x: isinstance(x, str) and x.strip() != '')]\n\n# Generate embeddings for the minimally processed text\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode(combined_df['minimal_processed_text'].tolist(), show_progress_bar=True)\n\n# Apply full preprocessing for BERTopic analysis\ncombined_df['processed_text'] = combined_df['selftext'].apply(full_preprocess)\n\n# Filter out empty fully processed texts\ncombined_df = combined_df[combined_df['processed_text'].apply(lambda x: isinstance(x, str) and x.strip() != '')]\n\n# Verify the shapes of documents and embeddings\nprint(f\"Number of documents: {len(combined_df['processed_text'])}\")\nprint(f\"Shape of embeddings: {embeddings.shape}\")\n\n# Check if they match\nif len(combined_df['processed_text']) != embeddings.shape[0]:\n    raise ValueError(\"The number of documents and the number of embeddings do not match.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T21:18:31.518391Z","iopub.execute_input":"2024-07-03T21:18:31.519145Z","iopub.status.idle":"2024-07-03T21:37:38.923580Z","shell.execute_reply.started":"2024-07-03T21:18:31.519107Z","shell.execute_reply":"2024-07-03T21:37:38.922624Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: bertopic in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: ndjson in /opt/conda/lib/python3.10/site-packages (0.3.1)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from bertopic) (1.26.4)\nRequirement already satisfied: hdbscan>=0.8.29 in /opt/conda/lib/python3.10/site-packages (from bertopic) (0.8.37)\nRequirement already satisfied: umap-learn>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from bertopic) (0.5.6)\nRequirement already satisfied: pandas>=1.1.5 in /opt/conda/lib/python3.10/site-packages (from bertopic) (2.2.1)\nRequirement already satisfied: scikit-learn>=0.22.2.post1 in /opt/conda/lib/python3.10/site-packages (from bertopic) (1.2.2)\nRequirement already satisfied: tqdm>=4.41.1 in /opt/conda/lib/python3.10/site-packages (from bertopic) (4.66.4)\nRequirement already satisfied: plotly>=4.7.0 in /opt/conda/lib/python3.10/site-packages (from bertopic) (5.18.0)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.41.2)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.23.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: cython<3,>=0.27 in /opt/conda/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (0.29.37)\nRequirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2023.4)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic) (8.2.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.2.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\nRequirement already satisfied: numba>=0.51.2 in /opt/conda/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\nRequirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic) (0.5.12)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/10008 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f471ba1ff67c4a1caf4098f87d62f95b"}},"metadata":{}},{"name":"stdout","text":"Number of documents: 320228\nShape of embeddings: (320228, 384)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Perform BERTopic analysis\ntopic_model = BERTopic()\ntopics, probs = topic_model.fit_transform(combined_df['processed_text'], embeddings)\n\n# Display the topics\nprint(topic_model.get_topic_info())\n\n# Visualize the topics\ntopic_model.visualize_topics()\n\n# Create and display the intertopic distance map\nfig = topic_model.visualize_heatmap()\nfig.show()\n\n# Create and display the hierarchical topic clusters using a dendrogram\nfig = topic_model.visualize_hierarchy()\nfig.show()\n\n# Save the BERTopic model to the desired path (e.g., Google Drive if needed)\nmodel_path = \"/kaggle/working/bertopic_model\"\ntopic_model.save(model_path)\nprint(f\"BERTopic model saved successfully at {model_path}.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-03T21:48:47.385468Z","iopub.execute_input":"2024-07-03T21:48:47.385756Z","iopub.status.idle":"2024-07-03T21:54:25.423926Z","shell.execute_reply.started":"2024-07-03T21:48:47.385727Z","shell.execute_reply":"2024-07-03T21:54:25.422353Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning:\n\nos.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform BERTopic analysis\u001b[39;00m\n\u001b[1;32m      2\u001b[0m topic_model \u001b[38;5;241m=\u001b[39m BERTopic()\n\u001b[0;32m----> 3\u001b[0m topics, probs \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m TOKENIZERS_PARALLELISM\u001b[38;5;241m=\u001b[39m(false)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Display the topics\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bertopic/_bertopic.py:411\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    408\u001b[0m umap_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce_dimensionality(embeddings, y)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Cluster reduced embeddings\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m documents, probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cluster_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mumap_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Sort and Map Topic IDs by their frequency\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_topics:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bertopic/_bertopic.py:3507\u001b[0m, in \u001b[0;36mBERTopic._cluster_embeddings\u001b[0;34m(self, umap_embeddings, documents, partial_fit, y)\u001b[0m\n\u001b[1;32m   3505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3506\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3507\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhdbscan_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mumap_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3508\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3509\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhdbscan_model\u001b[38;5;241m.\u001b[39mfit(umap_embeddings)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hdbscan/hdbscan_.py:1205\u001b[0m, in \u001b[0;36mHDBSCAN.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1195\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1196\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_kwargs)\n\u001b[1;32m   1198\u001b[0m (\n\u001b[1;32m   1199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_,\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobabilities_,\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_persistence_,\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condensed_tree,\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_linkage_tree,\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_min_spanning_tree,\n\u001b[0;32m-> 1205\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mhdbscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_finite:\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;66;03m# remap indices to align with original data in the case of non-finite entries.\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condensed_tree \u001b[38;5;241m=\u001b[39m remap_condensed_tree(\n\u001b[1;32m   1210\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condensed_tree, internal_to_raw, outliers\n\u001b[1;32m   1211\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hdbscan/hdbscan_.py:837\u001b[0m, in \u001b[0;36mhdbscan\u001b[0;34m(X, min_cluster_size, min_samples, alpha, cluster_selection_epsilon, max_cluster_size, metric, p, leaf_size, algorithm, memory, approx_min_span_tree, gen_min_span_tree, core_dist_n_jobs, cluster_selection_method, allow_single_cluster, match_reference_implementation, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m         (single_linkage_tree, result_min_span_tree) \u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39mcache(\n\u001b[1;32m    825\u001b[0m             _hdbscan_prims_kdtree\n\u001b[1;32m    826\u001b[0m         )(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 837\u001b[0m         (single_linkage_tree, result_min_span_tree) \u001b[38;5;241m=\u001b[39m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_hdbscan_boruvka_kdtree\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[43mleaf_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapprox_min_span_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgen_min_span_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcore_dist_n_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Metric is a valid BallTree metric\u001b[39;00m\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# TO DO: Need heuristic to decide when to go to boruvka;\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# still debugging for now\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m60\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hdbscan/hdbscan_.py:340\u001b[0m, in \u001b[0;36m_hdbscan_boruvka_kdtree\u001b[0;34m(X, min_samples, alpha, metric, p, leaf_size, approx_min_span_tree, gen_min_span_tree, core_dist_n_jobs, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    339\u001b[0m tree \u001b[38;5;241m=\u001b[39m KDTree(X, metric\u001b[38;5;241m=\u001b[39mmetric, leaf_size\u001b[38;5;241m=\u001b[39mleaf_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 340\u001b[0m alg \u001b[38;5;241m=\u001b[39m \u001b[43mKDTreeBoruvkaAlgorithm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleaf_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleaf_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapprox_min_span_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapprox_min_span_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_dist_n_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m min_spanning_tree \u001b[38;5;241m=\u001b[39m alg\u001b[38;5;241m.\u001b[39mspanning_tree()\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# Sort edges of the min_spanning_tree by weight\u001b[39;00m\n","File \u001b[0;32mhdbscan/_hdbscan_boruvka.pyx:392\u001b[0m, in \u001b[0;36mhdbscan._hdbscan_boruvka.KDTreeBoruvkaAlgorithm.__init__\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mhdbscan/_hdbscan_boruvka.pyx:426\u001b[0m, in \u001b[0;36mhdbscan._hdbscan_boruvka.KDTreeBoruvkaAlgorithm._compute_bounds\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Perform BERTopic analysis\ntopic_model = BERTopic()\ntopics, probs = topic_model.fit_transform(combined_df['processed_text'], embeddings)\n\n# Display the topics\nprint(topic_model.get_topic_info())\n\n# Visualize the topics\ntopic_model.visualize_topics()\n\n# # Verify the shapes of documents and embeddings\nprint(f\"Number of documents: {len(combined_df['processed_text'])}\")\nprint(f\"Shape of embeddings: {embeddings.shape}\")\n\n# Check if they match\nif len(combined_df['processed_text']) != embeddings.shape[0]:\n    raise ValueError(\"The number of documents and the number of embeddings do not match.\")\n\n# Perform BERTopic analysis\ntopic_model = BERTopic()\ntopics, probs = topic_model.fit_transform(combined_df['processed_text'], embeddings)\n\n# Display the topics\nprint(topic_model.get_topic_info())\n\n# Visualize the topics\ntopic_model.visualize_topics()\n\n# Create and display the intertopic distance map\nfig = topic_model.visualize_heatmap()\nfig.show()\n\n# Create and display the hierarchical topic clusters using a dendrogram\nfig = topic_model.visualize_hierarchy()\nfig.show()\n\n# Save the BERTopic model to the desired path (e.g., Google Drive if needed)\nmodel_path = \"/kaggle/working/bertopic_model\"\ntopic_model.save(model_path)\nprint(f\"BERTopic model saved successfully at {model_path}.\")Save the BERTopic model to the desired path (e.g., Google Drive if needed)\nmodel_path = \"/kaggle/working/bertopic_model\"\ntopic_model.save(model_path)\nprint(f\"BERTopic model saved successfully at {model_path}.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-03T21:10:38.493471Z","iopub.execute_input":"2024-07-03T21:10:38.493847Z","iopub.status.idle":"2024-07-03T21:10:38.503978Z","shell.execute_reply.started":"2024-07-03T21:10:38.493817Z","shell.execute_reply":"2024-07-03T21:10:38.502785Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[15], line 40\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"BERTopic model saved successfully at {model_path}.\")Save the BERTopic model to the desired path (e.g., Google Drive if needed)\u001b[0m\n\u001b[0m                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (2190777547.py, line 40)","output_type":"error"}]},{"cell_type":"code","source":"#create dendrogram and intertopic distance map for BERTopic model\n# Save the intertopic distance map\nfig = topic_model.visualize_heatmap()\nfig.write_html(\"/kaggle/working/intertopic_distance_map.html\")\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the hierarchical topic clusters dendrogram\nfig = topic_model.visualize_hierarchy()\nfig.write_html(\"/kaggle/working/hierarchical_topic_clusters.html\")","metadata":{},"execution_count":null,"outputs":[]}]}